# -*- coding: utf-8 -*-
"""darth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpfaHAKB94xQL1p8HWlz0zAO0MM1Qjdv

# Projeto Machine Learning Allcart - Product Categorization
Author: Ingrid Cadu<br>
Data: Sept 07, 2022<br>
<br>
The categorization takes the 3 last categories, however the 3rd column is discarted.<br>
E.g.<br>
Beverages › Beverage Syrups & Concentrates › Concentrates<br>
I have token:<br>
Beverages › Beverage Syrups & Concentrates›
"""

import re
import sys
import warnings
import pandas as pd
import numpy as np
import unicodedata
import html

# Stopwords package
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

# Split and vectorize them
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Library
url = 'https://media.githubusercontent.com/media/Ingrid-0906/PUP/main/CSVFiles/metadata_product.csv' # Check github data link
df = pd.read_csv(url)

"""# DEF: Function

## Building functions
"""

# Function to encode string
def encode_string(title):
  """(str) -> str
    Returns a string that is encoded as ascii
    :param title:
    :return:
  """
  try:
    encoded_title = unicodedata.normalize('NFKD', str(title)).encode('ascii', 'ignore')
    encoded_title = html.unescape(encoded_title.decode('ascii','ignore')).lower()
  except TypeError:  # if title is missing and a float
    encoded_title = 'NA'
    
  return encoded_title

# Tokenizing the title taking out the fancy thgs
def tokenize_title_string(title, excluded="‘.%/)("):
  """(str) -> list(str)
  Returns a list of string tokens given a string.
  It will exclude the following characters from the tokenization: - / . %
  :param title:
  :return:
  """
  striper = re.sub(r'[\b\d]+[a-z]+','',title)
  bag =  re.split("[^" + excluded + "\w]+", striper)
  #striper = re.sub(r'[\b\d]+[a-z]+','',bag)
  return [x for x in bag if x.strip()]

# Remove stopwords from string
def remove_words_list(sentence, words_to_remove):
  """ (list(str), set) -> list(str)
  Returns a list of tokens where the stopwords/spam words/colours have been removed
  :param title:
  :param words_to_remove:
  :return:
  >>> remove_words_list(['python', 'is', 'the', 'best'], set(stopwords.words('english')))
  ['python', 'best']
  """
  return [token for token in sentence if token not in words_to_remove]

# Remove words that are fully numeric
def remove_numeric_list(sentence):
  """ (list(str)) -> list(str)
  Remove words which are fully numeric
  :param title:
  :return:
  >>> remove_numeric_list(['A', 'B2', '1', '123', 'C'])
  ['A', 'B2', 'C']
  >>> remove_numeric_list(['1', '2', '3', '123'])
  []
  """
  return [token for token in sentence if not token.isdigit()]

# Remove common words from a sentence
stemmer = SnowballStemmer("english")
def stemming(sentence):
  """ (list(str)) -> list(str)
  Returns a list of str without stopwords.
  :parm title:
  :return:
  >>> stemming(['And','delicious'])
  ['delicious']
  """
  steem = [stemmer.stem(x) for x in sentence]
  return list(dict.fromkeys(steem))

# Remove words with character count below threshold from string
def remove_chars(sentence, word_len=2):
  """ (list(str), int) -> list(str)
  Returns a list of str (tokenized titles) where tokens of character length =< word_len is removed.     
  :param title:     
  :param word_len:     
  :return:     
  >>> remove_chars(['what', 'remains', 'of', 'a', 'word', '!', ''], 1)
  ['what', 'remains', 'of', 'word']
  """
  return [token for token in sentence if len(token) > word_len]

"""## Applying funtions"""

# Applying enconde function
low = [encode_string(x) for x in df['title']]

# Applying token function
clean = [tokenize_title_string(x,'') for x in low]

# Applying stopwords function / delayed 49s
words = [remove_words_list(x,set(stopwords.words('english'))) for x in clean]

# Applying no-numeric function
nums = [remove_numeric_list(x) for x in words]

# Applying stemm function / Delayed 34s
stemm = [stemming(x) for x in nums]

# Applying no-short words function
chars = [remove_chars(x) for x in stemm]

# Replacing in the whole datafra some characters that mess data
df['title'] = [' '.join(x) for x in chars]

# Getting just the first hierarchical position after mother
cat_0 = df['1'].str.strip().unique()
cat_1 = df['2'].str.strip().unique()

df_2 = np.concatenate([cat_0,cat_1])
df_2 = list(dict.fromkeys(df_2))
df_2 = [x for x in df_2 if x is not None]
len(df_2)

# Concating to create the matrix / delayed 27s and boosted
dxf = df.loc[:,['1','2']].copy()
df_3 = pd.concat([dxf, pd.DataFrame(columns=list(df_2))]).reset_index()
df_3 = df_3.rename(columns={'1':1, '2':2})
df_3.fillna(0, inplace=True)

# Creating a copy to compare after
df_4 = df_3.copy()

# loc is label-based, which means that you have to specify rows and columns based on their row and column labels / delayed 26m 14s
for i in range(1,3):
  row = 0
  for category in df_4[i]:
    if category!=0:
      df_4.loc[row,category] = 1
      # loc is label-based, which means that you have to specify rows and columns based on their row and column labels.
    row = row + 1

# Reseting index
df_1 = df.reset_index()

# creating new dataframe which contains name of product,description and categories it belong to
data_frame = pd.concat([df_1['title'], df_4.iloc[:,3:]],axis=1)

"""# Training and Testing data"""

# Splitting into samples to train & test
X_train, X_test, y_train, y_test = train_test_split(data_frame['title'], 
                                                   data_frame[data_frame.columns[1:]], 
                                                    test_size=0.3, 
                                                    random_state=0, 
                                                    shuffle=True)

# Using a tf-idf weighting scheme rather than normal boolean weights for better performance
vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2') 
vectorizer.fit(X_train)
X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)

# Shape of samples
print("X_train shape : ", X_train.shape)
print("X_test shape : ", X_test.shape)

